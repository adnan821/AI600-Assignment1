{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd432792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = pd.read_csv(\"data/processed/X_train_processed.csv\").values\n",
    "X_val = pd.read_csv(\"data/processed/X_val_processed.csv\").values\n",
    "\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\").values.flatten()\n",
    "y_val = pd.read_csv(\"data/processed/y_val.csv\").values.flatten()\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e00b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    one_hot_matrix = np.zeros((len(y), num_classes))\n",
    "    one_hot_matrix[np.arange(len(y)), y] = 1\n",
    "    return one_hot_matrix\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "y_train_oh = one_hot(y_train, num_classes)\n",
    "y_val_oh = one_hot(y_val, num_classes)\n",
    "print(\"One-hot target shape:\", y_train_oh)\n",
    "\n",
    "print(\"One-hot target shape:\", y_train_oh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78784030",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input dimension:\", X_train.shape[1])\n",
    "print(\"Unique classes:\", np.unique(y_train))\n",
    "print(\"Number of classes:\", len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1f873",
   "metadata": {},
   "source": [
    "# *MLP Implementation* #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "                       \n",
    "\n",
    "ip_dim = X_train.shape[1]                \n",
    "h1_dim = 64\n",
    "h2_dim = 32\n",
    "op_dim = len(np.unique(y_train))               \n",
    "\n",
    "\n",
    "print(\"\\nArchitecture:\")\n",
    "print(\"Input:\", ip_dim)\n",
    "print(\"Hidden 1:\", h1_dim)\n",
    "print(\"Hidden 2:\", h2_dim)\n",
    "print(\"Output:\", op_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93eb635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(ip_dim, h1_dim, h2_dim, op_dim):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    W1 = np.random.randn(ip_dim, h1_dim) * np.sqrt(2.0 / ip_dim)\n",
    "    b1 = np.zeros((1, h1_dim))\n",
    "    \n",
    "    W2 = np.random.randn(h1_dim, h2_dim) * np.sqrt(2.0 / h1_dim)\n",
    "    b2 = np.zeros((1, h2_dim))\n",
    "    \n",
    "    W3 = np.random.randn(h2_dim, op_dim) * np.sqrt(2.0 / h2_dim)\n",
    "    b3 = np.zeros((1, op_dim))\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde61535",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                    \n",
    "    \n",
    "                        \n",
    "    \n",
    "                          \n",
    "                                                                                           \n",
    "                                     \n",
    "    \n",
    "                          \n",
    "                                                                                               \n",
    "                                     \n",
    "    \n",
    "                         \n",
    "                                                                                             \n",
    "                                    \n",
    "    \n",
    "                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2, W3, b3 = initialize_parameters(\n",
    "    ip_dim, h1_dim, h2_dim, op_dim\n",
    ")\n",
    "\n",
    "print(\"W1 shape:\", W1.shape)\n",
    "print(\"W2 shape:\", W2.shape)\n",
    "print(\"W3 shape:\", W3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21519972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True)) \n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4420ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.random.randn(5, h1_dim)\n",
    "\n",
    "print(\"Sigmoid output shape:\", sigmoid(test_input).shape)\n",
    "print(\"ReLU output shape:\", relu(test_input).shape)\n",
    "print(\"Softmax output shape:\", softmax(np.random.randn(5, op_dim)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378578d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, b1, W2, b2, W3, b3, activation=\"sigmoid\"):\n",
    "    \n",
    "             \n",
    "    z1 = X @ W1 + b1\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        a1 = sigmoid(z1)\n",
    "    elif activation == \"relu\":\n",
    "        a1 = relu(z1)\n",
    "    else:\n",
    "        raise ValueError(\"Activation must be 'sigmoid' or 'relu'\")\n",
    "    \n",
    "             \n",
    "    z2 = a1 @ W2 + b2\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        a2 = sigmoid(z2)\n",
    "    elif activation == \"relu\":\n",
    "        a2 = relu(z2)\n",
    "    \n",
    "                  \n",
    "    z3 = a2 @ W3 + b3\n",
    "    y_hat = softmax(z3)\n",
    "    \n",
    "                                          \n",
    "    cache = {\n",
    "        \"X\": X,\n",
    "        \"z1\": z1,\n",
    "        \"a1\": a1,\n",
    "        \"z2\": z2,\n",
    "        \"a2\": a2,\n",
    "        \"z3\": z3,\n",
    "        \"y_hat\": y_hat\n",
    "    }\n",
    "    \n",
    "    return y_hat, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed397b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, cache = forward_pass(\n",
    "    X_train,\n",
    "    W1, b1, W2, b2, W3, b3,\n",
    "    activation=\"sigmoid\"\n",
    ")\n",
    "\n",
    "print(\"Output shape:\", y_hat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e80687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_hat):\n",
    "    \"\"\"\n",
    "    y_true: one-hot encoded labels (N, 4)\n",
    "    y_hat: softmax output (N, 4)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = y_true.shape[0]\n",
    "    \n",
    "                            \n",
    "    epsilon = 1e-15\n",
    "    y_hat_clipped = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    \n",
    "    loss = -np.sum(y_true * np.log(y_hat_clipped)) / N\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7fca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, _ = forward_pass(\n",
    "    X_train, W1, b1, W2, b2, W3, b3,\n",
    "    activation=\"sigmoid\"\n",
    ")\n",
    "\n",
    "loss = loss_function(y_train_oh, y_hat)\n",
    "print(\"Initial loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(y_true, cache, W1, W2, W3, activation=\"sigmoid\"):\n",
    "    \n",
    "    X = cache[\"X\"]\n",
    "    z1 = cache[\"z1\"]\n",
    "    a1 = cache[\"a1\"]\n",
    "    z2 = cache[\"z2\"]\n",
    "    a2 = cache[\"a2\"]\n",
    "    y_hat = cache[\"y_hat\"]\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    \n",
    "                              \n",
    "    dZ3 = y_hat - y_true\n",
    "    dW3 = (a2.T @ dZ3) / N\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / N\n",
    "    \n",
    "                                \n",
    "    dA2 = dZ3 @ W3.T\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ2 = dA2 * sigmoid_derivative(a2)\n",
    "    elif activation == \"relu\":\n",
    "        dZ2 = dA2 * relu_derivative(z2)\n",
    "    \n",
    "    dW2 = (a1.T @ dZ2) / N\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / N\n",
    "    \n",
    "                                \n",
    "    dA1 = dZ2 @ W2.T\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ1 = dA1 * sigmoid_derivative(a1)\n",
    "    elif activation == \"relu\":\n",
    "        dZ1 = dA1 * relu_derivative(z1)\n",
    "    \n",
    "    dW1 = (X.T @ dZ1) / N\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / N\n",
    "    \n",
    "    gradients = {\n",
    "        \"dW1\": dW1,\n",
    "        \"db1\": db1,\n",
    "        \"dW2\": dW2,\n",
    "        \"db2\": db2,\n",
    "        \"dW3\": dW3,\n",
    "        \"db3\": db3\n",
    "    }\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b06d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, cache = forward_pass(\n",
    "    X_train, W1, b1, W2, b2, W3, b3,\n",
    "    activation=\"sigmoid\"\n",
    ")\n",
    "\n",
    "grads = backprop(y_train_oh, cache, W1, W2, W3, activation=\"sigmoid\")\n",
    "\n",
    "print(\"dW1 shape:\", grads[\"dW1\"].shape)\n",
    "print(\"dW2 shape:\", grads[\"dW2\"].shape)\n",
    "print(\"dW3 shape:\", grads[\"dW3\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1015a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, W3, b3, gradients, learning_rate):\n",
    "    \n",
    "    W1 -= learning_rate * gradients[\"dW1\"]\n",
    "    b1 -= learning_rate * gradients[\"db1\"]\n",
    "    \n",
    "    W2 -= learning_rate * gradients[\"dW2\"]\n",
    "    b2 -= learning_rate * gradients[\"db2\"]\n",
    "    \n",
    "    W3 -= learning_rate * gradients[\"dW3\"]\n",
    "    b3 -= learning_rate * gradients[\"db3\"]\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X, y_true, W1, b1, W2, b2, W3, b3, activation):\n",
    "    y_hat, _ = forward_pass(X, W1, b1, W2, b2, W3, b3, activation)\n",
    "    predictions = np.argmax(y_hat, axis=1)\n",
    "    accuracy = np.mean(predictions == y_true)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af3484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X_train, y_train_oh, y_train,\n",
    "                  X_val, y_val_oh, y_val,\n",
    "                  activation=\"sigmoid\",\n",
    "                  learning_rate=0.1,\n",
    "                  iterations=200):\n",
    "    \n",
    "                           \n",
    "    W1, b1, W2, b2, W3, b3 = initialize_parameters(ip_dim, h1_dim, h2_dim, op_dim)\n",
    "    \n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    loss_list = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "                      \n",
    "        y_hat, cache = forward_pass(\n",
    "            X_train, W1, b1, W2, b2, W3, b3,\n",
    "            activation\n",
    "        )\n",
    "        \n",
    "                      \n",
    "        loss = loss_function(y_train_oh, y_hat)\n",
    "        \n",
    "                       \n",
    "        gradients = backprop(\n",
    "            y_train_oh, cache,\n",
    "            W1, W2, W3,\n",
    "            activation\n",
    "        )\n",
    "        \n",
    "                                                          \n",
    "        W1, b1, W2, b2, W3, b3 = update_params(\n",
    "            W1, b1, W2, b2, W3, b3,\n",
    "            gradients,\n",
    "            learning_rate\n",
    "        )\n",
    "        \n",
    "                       \n",
    "        train_acc = compute_accuracy(\n",
    "            X_train, y_train,\n",
    "            W1, b1, W2, b2, W3, b3,\n",
    "            activation\n",
    "        )\n",
    "        \n",
    "        val_acc = compute_accuracy(\n",
    "            X_val, y_val,\n",
    "            W1, b1, W2, b2, W3, b3,\n",
    "            activation\n",
    "        )\n",
    "        \n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "                                                      \n",
    "        if i % 20 == 0 or i == iterations - 1:\n",
    "            print(f\"Iteration {i} | Loss: {loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3, train_acc_list, val_acc_list, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25075847",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training with Sigmoid Activation...\")\n",
    "W1_s, b1_s, W2_s, b2_s, W3_s, b3_s, train_acc_sigmoid, val_acc_sigmoid, loss_sigmoid = train_network(\n",
    "    X_train, y_train_oh, y_train,\n",
    "    X_val, y_val_oh, y_val,\n",
    "    activation=\"sigmoid\",\n",
    "    learning_rate=0.01,\n",
    "    iterations=200\n",
    ")\n",
    "print(f\"\\nFinal Sigmoid - Train Acc: {train_acc_sigmoid[-1]:.4f}, Val Acc: {val_acc_sigmoid[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining with ReLU Activation...\")\n",
    "W1_r, b1_r, W2_r, b2_r, W3_r, b3_r, train_acc_relu, val_acc_relu, loss_relu = train_network(\n",
    "    X_train, y_train_oh, y_train,\n",
    "    X_val, y_val_oh, y_val,\n",
    "    activation=\"relu\",\n",
    "    learning_rate=0.01,\n",
    "    iterations=200\n",
    ")\n",
    "print(f\"\\nFinal ReLU - Train Acc: {train_acc_relu[-1]:.4f}, Val Acc: {val_acc_relu[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(train_acc_sigmoid, label=\"Train Sigmoid\")\n",
    "plt.plot(val_acc_sigmoid, label=\"Val Sigmoid\")\n",
    "\n",
    "plt.plot(train_acc_relu, label=\"Train ReLU\")\n",
    "plt.plot(val_acc_relu, label=\"Val ReLU\")\n",
    "\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_grad_tracking(X_train, y_train_oh, y_train,\n",
    "                             X_val, y_val_oh, y_val,\n",
    "                             activation=\"sigmoid\",\n",
    "                             learning_rate=0.1,\n",
    "                             iterations=200):\n",
    "    \n",
    "    W1, b1, W2, b2, W3, b3 = initialize_parameters(ip_dim, h1_dim, h2_dim, op_dim)\n",
    "    \n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    loss_list = []\n",
    "    grad_W1_list = []\n",
    "    grad_W2_list = []\n",
    "    grad_W3_list = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        y_hat, cache = forward_pass(X_train, W1, b1, W2, b2, W3, b3, activation)\n",
    "        loss = loss_function(y_train_oh, y_hat)\n",
    "        gradients = backprop(y_train_oh, cache, W1, W2, W3, activation)\n",
    "        \n",
    "                                   \n",
    "        grad_W1_list.append(np.mean(np.abs(gradients[\"dW1\"])))\n",
    "        grad_W2_list.append(np.mean(np.abs(gradients[\"dW2\"])))\n",
    "        grad_W3_list.append(np.mean(np.abs(gradients[\"dW3\"])))\n",
    "        \n",
    "        W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, gradients, learning_rate)\n",
    "        \n",
    "        train_acc = compute_accuracy(X_train, y_train, W1, b1, W2, b2, W3, b3, activation)\n",
    "        val_acc = compute_accuracy(X_val, y_val, W1, b1, W2, b2, W3, b3, activation)\n",
    "        \n",
    "        train_acc_list.append(train_acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        if i % 20 == 0 or i == iterations - 1:\n",
    "            print(f\"Iteration {i} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return (W1, b1, W2, b2, W3, b3, train_acc_list, val_acc_list, loss_list, \n",
    "            grad_W1_list, grad_W2_list, grad_W3_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651de0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            \n",
    "print(\"Training with Gradient Tracking...\")\n",
    "print(\"\\n--- Sigmoid ---\")\n",
    "results_sigmoid = train_with_grad_tracking(\n",
    "    X_train, y_train_oh, y_train,\n",
    "    X_val, y_val_oh, y_val,\n",
    "    activation=\"sigmoid\",\n",
    "    learning_rate=0.01,\n",
    "    iterations=200\n",
    ")\n",
    "\n",
    "print(\"\\n--- ReLU ---\")\n",
    "results_relu = train_with_grad_tracking(\n",
    "    X_train, y_train_oh, y_train,\n",
    "    X_val, y_val_oh, y_val,\n",
    "    activation=\"relu\",\n",
    "    learning_rate=0.01,\n",
    "    iterations=200\n",
    ")\n",
    "\n",
    "print(\"\\nGradient tracking complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "                             \n",
    "grad_W1_sigmoid = results_sigmoid[9]\n",
    "grad_W2_sigmoid = results_sigmoid[10]\n",
    "grad_W3_sigmoid = results_sigmoid[11]\n",
    "\n",
    "grad_W1_relu = results_relu[9]\n",
    "grad_W2_relu = results_relu[10]\n",
    "grad_W3_relu = results_relu[11]\n",
    "\n",
    "                                        \n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "              \n",
    "axes[0].plot(grad_W1_sigmoid, label='Sigmoid', linewidth=2, linestyle='-', alpha=0.8)\n",
    "axes[0].plot(grad_W1_relu, label='ReLU', linewidth=2, linestyle='--', alpha=0.8)\n",
    "axes[0].set_xlabel('Iterations', fontsize=12)\n",
    "axes[0].set_ylabel('Mean Gradient Magnitude', fontsize=12)\n",
    "axes[0].set_title('Layer 1 (W1): Gradient Magnitude Over Training', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "              \n",
    "axes[1].plot(grad_W2_sigmoid, label='Sigmoid', linewidth=2, linestyle='-', alpha=0.8)\n",
    "axes[1].plot(grad_W2_relu, label='ReLU', linewidth=2, linestyle='--', alpha=0.8)\n",
    "axes[1].set_xlabel('Iterations', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Gradient Magnitude', fontsize=12)\n",
    "axes[1].set_title('Layer 2 (W2): Gradient Magnitude Over Training', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "                   \n",
    "axes[2].plot(grad_W3_sigmoid, label='Sigmoid', linewidth=2, linestyle='-', alpha=0.8)\n",
    "axes[2].plot(grad_W3_relu, label='ReLU', linewidth=2, linestyle='--', alpha=0.8)\n",
    "axes[2].set_xlabel('Iterations', fontsize=12)\n",
    "axes[2].set_ylabel('Mean Gradient Magnitude', fontsize=12)\n",
    "axes[2].set_title('Output (W3): Gradient Magnitude Over Training', fontsize=13, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GRADIENT ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Sigmoid - Final Layer 1 grad: {grad_W1_sigmoid[-1]:.2e}\")\n",
    "print(f\"Sigmoid - Final Layer 2 grad: {grad_W2_sigmoid[-1]:.2e}\")\n",
    "print(f\"Sigmoid - Final Output grad:  {grad_W3_sigmoid[-1]:.2e}\")\n",
    "print()\n",
    "print(f\"ReLU    - Final Layer 1 grad: {grad_W1_relu[-1]:.2e}\")\n",
    "print(f\"ReLU    - Final Layer 2 grad: {grad_W2_relu[-1]:.2e}\")\n",
    "print(f\"ReLU    - Final Output grad:  {grad_W3_relu[-1]:.2e}\")\n",
    "print(\"=\"*70)\n",
    "print(\"Note: Sigmoid shows much smaller gradients (vanishing gradient problem)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"data/raw/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                \n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "numerical_cols = ['minimum_nights', 'amenity_score', 'number_of_reviews', 'availability_365']\n",
    "categorical_cols = ['neighbourhood_group', 'room_type']\n",
    "\n",
    "                    \n",
    "for i, col in enumerate(numerical_cols):\n",
    "    axes[i].hist(test_df[col], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "                      \n",
    "for i, col in enumerate(categorical_cols, start=len(numerical_cols)):\n",
    "    test_df[col].value_counts().plot(kind='bar', ax=axes[i], color='coral', edgecolor='black')\n",
    "    axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "                 \n",
    "test_df['price_class'].value_counts().sort_index().plot(kind='bar', ax=axes[6], \n",
    "                                                         color='mediumseagreen', edgecolor='black')\n",
    "axes[6].set_title('Distribution of price_class (Target)', fontsize=12, fontweight='bold')\n",
    "axes[6].set_xlabel('Price Class')\n",
    "axes[6].set_ylabel('Count')\n",
    "axes[6].tick_params(axis='x', rotation=0)\n",
    "axes[6].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "                              \n",
    "axes[7].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "                          \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST DATA - SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNumerical Features:\")\n",
    "print(test_df[numerical_cols].describe())\n",
    "print(\"\\nCategorical Features:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(test_df[col].value_counts())\n",
    "print(\"\\nTarget Variable (price_class):\")\n",
    "print(test_df['price_class'].value_counts().sort_index())\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c959087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"data/raw/test.csv\")\n",
    "\n",
    "                                                                                               \n",
    "                                                         \n",
    "\n",
    "train_df = pd.read_csv(\"data/raw/train.csv\")\n",
    "median_values = train_df[numerical_cols].median()\n",
    "\n",
    "test_df[numerical_cols] = test_df[numerical_cols].fillna(median_values)\n",
    "test_df[categorical_cols] = test_df[categorical_cols].fillna(\"Unknown\")\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoder.fit(train_df[categorical_cols].fillna(\"Unknown\"))\n",
    "\n",
    "X_test_encoded = encoder.transform(test_df[categorical_cols])\n",
    "\n",
    "X_test_encoded_df = pd.DataFrame(\n",
    "    X_test_encoded,\n",
    "    columns=encoder.get_feature_names_out(categorical_cols)\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[numerical_cols].fillna(median_values))\n",
    "\n",
    "X_test_scaled = scaler.transform(test_df[numerical_cols])\n",
    "\n",
    "X_test_scaled_df = pd.DataFrame(\n",
    "    X_test_scaled,\n",
    "    columns=numerical_cols\n",
    ")\n",
    "\n",
    "X_test_final = pd.concat(\n",
    "    [X_test_scaled_df, X_test_encoded_df],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "X_test = X_test_final.values\n",
    "\n",
    "print(\"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df[\"price_class\"].values\n",
    "\n",
    "test_accuracy_relu = compute_accuracy(X_test, y_test, W1_r, b1_r, W2_r, b2_r, W3_r, b3_r, activation=\"relu\")\n",
    "test_accuracy_sigmoid = compute_accuracy(X_test, y_test, W1_s, b1_s, W2_s, b2_s, W3_s, b3_s, activation=\"sigmoid\")\n",
    "\n",
    "print(\"Test Accuracy for ReLU:\", test_accuracy_relu)\n",
    "print(\"Test Accuracy for Sigmoid:\", test_accuracy_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "                                 \n",
    "y_hat_sigmoid, _ = forward_pass(X_test, W1_s, b1_s, W2_s, b2_s, W3_s, b3_s, activation=\"sigmoid\")\n",
    "y_pred_sigmoid = np.argmax(y_hat_sigmoid, axis=1)\n",
    "\n",
    "y_hat_relu, _ = forward_pass(X_test, W1_r, b1_r, W2_r, b2_r, W3_r, b3_r, activation=\"relu\")\n",
    "y_pred_relu = np.argmax(y_hat_relu, axis=1)\n",
    "\n",
    "                            \n",
    "cm_sigmoid = confusion_matrix(y_test, y_pred_sigmoid)\n",
    "cm_relu = confusion_matrix(y_test, y_pred_relu)\n",
    "\n",
    "                         \n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "                          \n",
    "sns.heatmap(cm_sigmoid, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=range(4), yticklabels=range(4), cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Confusion Matrix - Sigmoid Activation', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "\n",
    "                       \n",
    "sns.heatmap(cm_relu, annot=True, fmt='d', cmap='Greens', ax=axes[1], \n",
    "            xticklabels=range(4), yticklabels=range(4), cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('Confusion Matrix - ReLU Activation', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "                              \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SIGMOID - CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(\n",
    "    y_test, y_pred_sigmoid,\n",
    "    target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3'],\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ReLU - CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(\n",
    "    y_test, y_pred_relu,\n",
    "    target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3'],\n",
    "    zero_division=0\n",
    ") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4e709",
   "metadata": {},
   "source": [
    "## Part B(b): Gradient Magnitude Analysis\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "#### **1. Vanishing Gradients in Sigmoid:**\n",
    "- **Layer 1 (W1)**: Gradient magnitude ≈ 5.62×10⁻⁴ (extremely small)\n",
    "- **Layer 2 (W2)**: Gradient magnitude ≈ 1.39×10⁻³ (small)\n",
    "- **Output (W3)**: Gradient magnitude ≈ 5.09×10⁻³ (larger)\n",
    "\n",
    "**Pattern**: Gradients **decrease** as we move backward through layers (left to right in backprop). Earlier layers receive much smaller gradient signals.\n",
    "\n",
    "#### **2. Stable Gradients in ReLU:**\n",
    "- **Layer 1 (W1)**: Gradient magnitude ≈ 2.40×10⁻³ (4-5x larger than Sigmoid)\n",
    "- **Layer 2 (W2)**: Gradient magnitude ≈ 3.63×10⁻³ (similar scale across layers)\n",
    "- **Output (W3)**: Gradient magnitude ≈ 1.08×10⁻² (more balanced across layers)\n",
    "\n",
    "**Pattern**: ReLU maintains **consistent gradient magnitudes** across all layers.\n",
    "\n",
    "### What This Reveals About Gradient Flow and Learning Dynamics:\n",
    "\n",
    "#### **1. Sigmoid - The Vanishing Gradient Problem:**\n",
    "- Sigmoid's derivative $(a(1-a))$ has maximum value of 0.25\n",
    "- During backpropagation, gradients are multiplied by these small derivatives at each layer\n",
    "- Result: Exponential decay of gradients moving backward → $\\text{grad} \\times 0.25 \\times 0.25 \\times 0.25 = 0.0156 \\times \\text{grad}$\n",
    "- **Impact on learning**: Early layers receive negligible gradient signals, making weight updates impossible\n",
    "- **Consequence**: Model cannot learn meaningful patterns in early layers, gets stuck at baseline (56% = majority class)\n",
    "\n",
    "#### **2. ReLU - Superior Gradient Flow:**\n",
    "- ReLU derivative is simply 1 (for $z > 0$) or 0 (for $z \\leq 0$)\n",
    "- No multiplicative attenuation: gradients flow back unchanged through active neurons\n",
    "- Result: Gradients maintain magnitude across layers\n",
    "- **Impact on learning**: All layers receive meaningful gradient signals\n",
    "- **Consequence**: Model can learn complex hierarchical representations\n",
    "\n",
    "#### **3. Deep Networks Implications:**\n",
    "- **Sigmoid in Deep Networks**: Becomes practically unusable beyond 2-3 layers\n",
    "- **ReLU in Deep Networks**: Enables training of very deep architectures (ResNet: 152+ layers)\n",
    "- **Why it matters**: This is why modern deep learning uses ReLU variants (Leaky ReLU, ELU, GELU) instead of sigmoid for hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c252ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "                         \n",
    "X_train_df = pd.read_csv(\"data/processed/X_train_processed.csv\")\n",
    "X_val_df = pd.read_csv(\"data/processed/X_val_processed.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\").values.flatten()\n",
    "y_val = pd.read_csv(\"data/processed/y_val.csv\").values.flatten()\n",
    "\n",
    "                                                  \n",
    "feature_names = X_train_df.columns.tolist()\n",
    "\n",
    "                      \n",
    "X_train = X_train_df.values\n",
    "X_val = X_val_df.values\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Number of features:\", len(feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                 \n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Input dimension:\", input_dim)\n",
    "print(\"Number of classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2690e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "                      \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden1: int = 64,\n",
    "        hidden2: int = 32,\n",
    "        num_classes: int = 4,\n",
    "        activation: str = \"relu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, num_classes)\n",
    "\n",
    "        activation = activation.lower()\n",
    "        if activation == \"relu\":\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation_fn = nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(\"activation must be 'relu' or 'sigmoid'\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.activation_fn(self.fc2(x))\n",
    "        x = self.fc3(x)                                                \n",
    "        return x\n",
    "\n",
    "\n",
    "                               \n",
    "model_relu = MLP(input_dim=input_dim, num_classes=num_classes, activation=\"relu\")\n",
    "model_sigmoid = MLP(input_dim=input_dim, num_classes=num_classes, activation=\"sigmoid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279cd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "                  \n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "                                           \n",
    "optimizers = {\n",
    "    \"relu\": optim.Adam(model_relu.parameters(), lr=0.001),\n",
    "    \"sigmoid\": optim.Adam(model_sigmoid.parameters(), lr=0.001),\n",
    "}\n",
    "\n",
    "                                         \n",
    "                                                                 \n",
    "model_name = \"relu\"                                             \n",
    "model = model_relu if model_name == \"relu\" else model_sigmoid\n",
    "optimizer = optimizers[model_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(model, optimizer, model_name, epochs=200):\n",
    "    train_loss_hist, train_acc_hist, val_acc_hist = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb)\n",
    "            loss = criteria(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(X_train_tensor)\n",
    "            train_preds = torch.argmax(train_outputs, dim=1)\n",
    "            train_acc = (train_preds == y_train_tensor).float().mean().item()\n",
    "\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_preds = torch.argmax(val_outputs, dim=1)\n",
    "            val_acc = (val_preds == y_val_tensor).float().mean().item()\n",
    "\n",
    "        train_loss_hist.append(avg_loss)\n",
    "        train_acc_hist.append(train_acc)\n",
    "        val_acc_hist.append(val_acc)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"[{model_name}] Epoch [{epoch+1}/{epochs}] | \"\n",
    "                f\"Loss: {avg_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    return train_loss_hist, train_acc_hist, val_acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee229c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                     \n",
    "\n",
    "epochs = 200\n",
    "\n",
    "                                                \n",
    "                                                                                         \n",
    "                                                                                   \n",
    "\n",
    "                \n",
    "                                                                  \n",
    "                                                            \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "print(\"Training SIGMOID model...\")\n",
    "loss_sigmoid_torch, train_acc_sigmoid_torch, val_acc_sigmoid_torch = train_one_model(\n",
    "    model_sigmoid, optimizers[\"sigmoid\"], \"sigmoid\", epochs=epochs\n",
    ")\n",
    "\n",
    "print(\"\\nTraining RELU model...\")\n",
    "loss_relu_torch, train_acc_relu_torch, val_acc_relu_torch = train_one_model(\n",
    "    model_relu, optimizers[\"relu\"], \"relu\", epochs=epochs\n",
    ")\n",
    "\n",
    "                                                            \n",
    "model_name = \"relu\"\n",
    "model = model_relu\n",
    "optimizer = optimizers[\"relu\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "                     \n",
    "\n",
    "model.eval()\n",
    "\n",
    "                                    \n",
    "X_for_grad = X_train_tensor.clone().detach().requires_grad_(True)\n",
    "\n",
    "outputs = model(X_for_grad)\n",
    "loss = criteria(outputs, y_train_tensor)\n",
    "\n",
    "                      \n",
    "model.zero_grad()\n",
    "\n",
    "                           \n",
    "loss.backward()\n",
    "\n",
    "                              \n",
    "input_gradients = X_for_grad.grad                            \n",
    "\n",
    "                                               \n",
    "feature_importance = torch.mean(torch.abs(input_gradients), dim=0)\n",
    "\n",
    "feature_importance = feature_importance.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c77c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                       \n",
    "ranking = sorted(\n",
    "    zip(feature_names, feature_importance),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"Feature Importance Ranking (Highest → Lowest):\\n\")\n",
    "\n",
    "for i, (name, score) in enumerate(ranking, 1):\n",
    "    print(f\"{i:2d}. {name:30s} {score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                \n",
    "\n",
    "print(\"\\nTop 5 Most Influential Features:\\n\")\n",
    "\n",
    "for i, (name, score) in enumerate(ranking[:5], 1):\n",
    "    print(f\"{i}. {name:30s} {score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                 \n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "sorted_features = [x[0] for x in ranking]\n",
    "sorted_scores = [x[1] for x in ranking]\n",
    "\n",
    "plt.barh(sorted_features[::-1], sorted_scores[::-1])\n",
    "plt.xlabel(\"Average |∂L/∂x_i|\")\n",
    "plt.title(\"Gradient-Based Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \n",
    "\n",
    "normalized_importance = feature_importance / np.sum(feature_importance)\n",
    "\n",
    "ranking_norm = sorted(zip(feature_names, normalized_importance),\n",
    "                      key=lambda x: x[1],\n",
    "                      reverse=True)\n",
    "\n",
    "print(\"Normalized Importance Ranking:\\n\")\n",
    "for name, score in ranking_norm:\n",
    "    print(f\"{name:40s} {score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b4dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "                                                         \n",
    "model_relu.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model_relu(X_test_tensor)\n",
    "    test_preds = torch.argmax(test_outputs, dim=1)\n",
    "    test_acc_relu_torch = (test_preds == y_test_tensor).float().mean().item()\n",
    "\n",
    "                       \n",
    "model_sigmoid.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs_sigmoid = model_sigmoid(X_test_tensor)\n",
    "    test_preds_sigmoid = torch.argmax(test_outputs_sigmoid, dim=1)\n",
    "    test_acc_sigmoid_torch = (test_preds_sigmoid == y_test_tensor).float().mean().item()\n",
    "\n",
    "                                                       \n",
    "test_acc = test_acc_relu_torch\n",
    "\n",
    "print(f\"Test Accuracy (ReLU): {test_acc_relu_torch:.4f}\")\n",
    "print(f\"Test Accuracy (Sigmoid): {test_acc_sigmoid_torch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm_relu = confusion_matrix(y_test_tensor.numpy(), test_preds.numpy())\n",
    "cm_sigmoid = confusion_matrix(y_test_tensor.numpy(), test_preds_sigmoid.numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_relu, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "    xticklabels=[\"0\", \"1\", \"2\", \"3\"],\n",
    "    yticklabels=[\"0\", \"1\", \"2\", \"3\"],\n",
    "    ax=axes[0]\n",
    " )\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"True\")\n",
    "axes[0].set_title(\"Confusion Matrix - PyTorch ReLU\")\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_sigmoid, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
    "    xticklabels=[\"0\", \"1\", \"2\", \"3\"],\n",
    "    yticklabels=[\"0\", \"1\", \"2\", \"3\"],\n",
    "    ax=axes[1]\n",
    " )\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"True\")\n",
    "axes[1].set_title(\"Confusion Matrix - PyTorch Sigmoid\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31955e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "                           \n",
    "import pandas as pd\n",
    "\n",
    "accuracy_table = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Custom MLP (Sigmoid)\",\n",
    "        \"Custom MLP (ReLU)\",\n",
    "        \"PyTorch MLP (ReLU)\",\n",
    "        \"PyTorch MLP (Sigmoid)\",\n",
    "    ],\n",
    "    \"Test Accuracy\": [\n",
    "        test_accuracy_sigmoid,\n",
    "        test_accuracy_relu,\n",
    "        test_acc,\n",
    "        test_acc_sigmoid_torch,\n",
    "    ],\n",
    "})\n",
    "\n",
    "accuracy_table[\"Test Accuracy\"] = accuracy_table[\"Test Accuracy\"].round(4)\n",
    "accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nClassification Report (PyTorch ReLU):\\n\")\n",
    "print(classification_report(\n",
    "    y_test_tensor.numpy(),\n",
    "    test_preds.numpy(),\n",
    "    target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"],\n",
    "    zero_division=0\n",
    ") )\n",
    "\n",
    "print(\"\\nClassification Report (PyTorch Sigmoid):\\n\")\n",
    "print(classification_report(\n",
    "    y_test_tensor.numpy(),\n",
    "    test_preds_sigmoid.numpy(),\n",
    "    target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"],\n",
    "    zero_division=0\n",
    ") )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
